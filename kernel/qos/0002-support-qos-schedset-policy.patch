From 181e6ea8b7d5ea61fb80f91d6eedd80f33c96aff Mon Sep 17 00:00:00 2001
From: zou cao <zoucaox@outlook.com>
Date: Wed, 18 May 2022 11:53:28 +0800
Subject: [PATCH 2/4] support qos  schedset policy

Signed-off-by: zou cao <zoucaox@outlook.com>
---
 include/linux/sched.h      | 19 +++++++++
 include/linux/sched/prio.h |  2 +
 include/linux/sched/rt.h   |  7 ++++
 include/uapi/linux/sched.h |  1 +
 kernel/sched/Makefile      |  2 +-
 kernel/sched/core.c        | 16 +++++++-
 kernel/sched/qos.c         | 84 +++++++++++++++++++++++++++++++++++++-
 kernel/sched/rt.c          |  3 +-
 kernel/sched/sched.h       | 50 ++++++++++++++++++++++-
 9 files changed, 178 insertions(+), 6 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8055f43d8bbf..6fce5d6c70f8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -515,6 +515,23 @@ struct sched_entity {
 	CK_HOTFIX_RESERVE(2)
 };
 
+struct sched_qos_entity {
+	struct list_head		run_list;
+	unsigned long			timeout;
+	unsigned long			watchdog_stamp;
+	unsigned int			time_slice;
+	unsigned short			on_rq;
+	unsigned short			on_list;
+
+	struct sched_rt_entity		*back;
+	struct sched_rt_entity		*parent;
+	/* rq on which this entity is (to be) queued: */
+	struct qos_rq			*qos_rq;
+	/* rq "owned" by this entity/group: */
+	struct qos_rq			*my_q;
+} __randomize_layout;
+
+
 struct sched_rt_entity {
 	struct list_head		run_list;
 	unsigned long			timeout;
@@ -675,10 +692,12 @@ struct task_struct {
 	int				static_prio;
 	int				normal_prio;
 	unsigned int			rt_priority;
+	unsigned int			qos_priority;
 
 	const struct sched_class	*sched_class;
 	struct sched_entity		se;
 	struct sched_rt_entity		rt;
+	struct sched_qos_entity		qos;
 #ifdef CONFIG_CGROUP_SCHED
 	struct task_group		*sched_task_group;
 #endif
diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index 7d64feafc408..d26725f5b0fd 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -19,7 +19,9 @@
  * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
  */
 
+#define MAX_USER_QOS_PRIO	10
 #define MAX_USER_RT_PRIO	100
+#define MAX_QOS_PRIO	MAX_USER_QOS_PRIO
 #define MAX_RT_PRIO		MAX_USER_RT_PRIO
 
 #define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
diff --git a/include/linux/sched/rt.h b/include/linux/sched/rt.h
index e5af028c08b4..024fd8a1b86a 100644
--- a/include/linux/sched/rt.h
+++ b/include/linux/sched/rt.h
@@ -6,6 +6,13 @@
 
 struct task_struct;
 
+static inline int qos_prio(int prio)
+{
+	if (unlikely(prio > 0 && prio < MAX_QOS_PRIO))
+		return 1;
+	return 0;
+}
+
 static inline int rt_prio(int prio)
 {
 	if (unlikely(prio < MAX_RT_PRIO))
diff --git a/include/uapi/linux/sched.h b/include/uapi/linux/sched.h
index ed4ee170bee2..4b5aa676e452 100644
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@ -41,6 +41,7 @@
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
 #define SCHED_DEADLINE		6
+#define SCHED_QOS		7	
 
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 21fb5a5662b5..7a7ad7f00f6a 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -17,7 +17,7 @@ CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
 obj-y += core.o loadavg.o clock.o cputime.o
-obj-y += idle.o fair.o rt.o deadline.o
+obj-y += idle.o fair.o rt.o deadline.o qos.o
 obj-y += wait.o wait_bit.o swait.o completion.o
 
 obj-$(CONFIG_SMP) += cpupri.o cpudeadline.o topology.o stop_task.o pelt.o
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 60fd42eb46d6..38ed0112807f 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -801,6 +801,8 @@ static inline int normal_prio(struct task_struct *p)
 
 	if (task_has_dl_policy(p))
 		prio = MAX_DL_PRIO-1;
+	else if (task_has_qos_policy(p))
+		prio = MAX_QOS_PRIO-1 - p->qos_priority;
 	else if (task_has_rt_policy(p))
 		prio = MAX_RT_PRIO-1 - p->rt_priority;
 	else
@@ -4112,6 +4114,8 @@ static void __setscheduler_params(struct task_struct *p,
 		__setparam_dl(p, attr);
 	else if (fair_policy(policy))
 		p->static_prio = NICE_TO_PRIO(attr->sched_nice);
+	else if (qos_policy(policy))
+		p->qos_priority = attr->sched_priority;
 
 	/*
 	 * __sched_setscheduler() ensures attr->sched_priority == 0 when
@@ -4137,12 +4141,17 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 	if (keep_boost)
 		p->prio = rt_effective_prio(p, p->prio);
 
+	printk("zz %s %d %d %d\n", __func__, __LINE__, p->prio, qos_prio(p->prio));
+
 	if (dl_prio(p->prio))
 		p->sched_class = &dl_sched_class;
+	else if (qos_prio(p->prio))
+		p->sched_class = &qos_sched_class;
 	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
+
 }
 
 /*
@@ -4201,7 +4210,7 @@ static int __sched_setscheduler(struct task_struct *p,
 	    (!p->mm && attr->sched_priority > MAX_RT_PRIO-1))
 		return -EINVAL;
 	if ((dl_policy(policy) && !__checkparam_dl(attr)) ||
-	    (rt_policy(policy) != (attr->sched_priority != 0)))
+	    ((rt_policy(policy) != (attr->sched_priority != 0)) && !qos_policy(policy)))
 		return -EINVAL;
 
 	/*
@@ -4291,6 +4300,8 @@ static int __sched_setscheduler(struct task_struct *p,
 			goto change;
 		if (rt_policy(policy) && attr->sched_priority != p->rt_priority)
 			goto change;
+		if (qos_policy(policy) && attr->sched_priority != p->qos_priority)
+			goto change;
 		if (dl_policy(policy) && dl_param_changed(p, attr))
 			goto change;
 
@@ -4383,6 +4394,7 @@ static int __sched_setscheduler(struct task_struct *p,
 		if (oldprio < p->prio)
 			queue_flags |= ENQUEUE_HEAD;
 
+		printk("zz %s %d %lx\n", __func__, __LINE__, p->sched_class);
 		enqueue_task(rq, p, queue_flags);
 	}
 	if (running)
@@ -5204,6 +5216,7 @@ SYSCALL_DEFINE1(sched_get_priority_max, int, policy)
 	switch (policy) {
 	case SCHED_FIFO:
 	case SCHED_RR:
+	case SCHED_QOS:
 		ret = MAX_USER_RT_PRIO-1;
 		break;
 	case SCHED_DEADLINE:
@@ -5231,6 +5244,7 @@ SYSCALL_DEFINE1(sched_get_priority_min, int, policy)
 	switch (policy) {
 	case SCHED_FIFO:
 	case SCHED_RR:
+	case SCHED_QOS:
 		ret = 1;
 		break;
 	case SCHED_DEADLINE:
diff --git a/kernel/sched/qos.c b/kernel/sched/qos.c
index 434404e7e803..3cd2bb815156 100644
--- a/kernel/sched/qos.c
+++ b/kernel/sched/qos.c
@@ -7,6 +7,17 @@
 
 #include "pelt.h"
 
+#define DEBUGLEVE 1
+
+#define level_debug(level, fmt, arg...) \
+    if (level >= DEBUGLEVE) \
+        printk(fmt, ##arg);
+
+#define MEDEBUG(fmt, args...) level_debug(1, fmt, ## args)
+#define MEINFO(fmt,args...)     level_debug(2, fmt, ## args)
+#define MEWARN(fmt,args...)     level_debug(3, fmt, ## args)
+#define MEERR(fmt,args...)  level_debug(4, fmt, ## args)
+
 static void enqueue_task_qos(struct rq *rq, struct task_struct *p, int flags);
 static void dequeue_task_qos(struct rq *rq, struct task_struct *p, int flags);
 static void yield_task_qos(struct rq *rq);
@@ -21,12 +32,13 @@ static void switched_from_qos(struct rq *rq, struct task_struct *p);
 static void set_curr_task_qos(struct rq *rq);
 static void task_tick_qos(struct rq *rq, struct task_struct *p, int queued);
 static unsigned int get_rr_interval_qos(struct rq *rq, struct task_struct *task);
+#ifdef CONFIG_SCHED_SLI
 static void update_nr_uninterruptible_qos(struct task_struct *p, long inc);
+#endif
 static void prio_changed_qos(struct rq *rq, struct task_struct *p, int oldprio);
 static void switched_to_qos(struct rq *rq, struct task_struct *p);
 static void update_curr_qos(struct rq *rq);
 
-
 const struct sched_class qos_sched_class = {
 	.next			= &fair_sched_class,
 	.enqueue_task		= enqueue_task_qos,
@@ -63,20 +75,67 @@ const struct sched_class qos_sched_class = {
 #endif
 };
 
+static inline struct qos_rq *group_qos_rq(struct sched_qos_entity *qos_se)
+{
+	return qos_se->my_q;
+}
+
+static inline struct task_struct *qos_task_of(struct sched_qos_entity *qos_se)
+{
+	return container_of(qos_se, struct task_struct, qos);
+}
+
+static inline struct rq *rq_of_qos_rq(struct qos_rq *qos_rq)
+{
+	return qos_rq->rq;
+}
+
+static inline struct qos_rq *qos_rq_of_se(struct sched_qos_entity *qos_se)
+{
+	return qos_se->qos_rq;
+}
+
+static inline struct rq *rq_of_qos_se(struct sched_qos_entity *qos_se)
+{
+	struct qos_rq *qos_rq = qos_se->qos_rq;
+
+	return qos_rq->rq;
+}
+
+static inline int qos_se_prio(struct sched_qos_entity *qos_se)
+{
+	struct qos_rq *qos_rq = group_qos_rq(qos_se);
+
+	if (qos_rq)
+		return qos_rq->highest_prio.curr;
+
+	return qos_task_of(qos_se)->prio;
+}
 /*
  * Adding/removing a task to/from a priority array:
  */
 static void
 enqueue_task_qos(struct rq *rq, struct task_struct *p, int flags)
 {
+	struct sched_qos_entity *qos_se = &p->qos;
+	struct qos_rq *qos_rq = qos_se->qos_rq;
+	struct qos_prio_array *array = &qos_rq->active;
+	struct list_head *queue = array->queue + qos_se_prio(qos_se);
+
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
+	list_add_tail(&qos_se->run_list, queue);
+
+	//return qos_rq->rq;
 }
 
 static void dequeue_task_qos(struct rq *rq, struct task_struct *p, int flags)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
 static void yield_task_qos(struct rq *rq)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
 /*
@@ -84,53 +143,67 @@ static void yield_task_qos(struct rq *rq)
  */
 static void check_preempt_curr_qos(struct rq *rq, struct task_struct *p, int flags)
 {
-
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
+#if 0
 static struct sched_qos_entity *pick_next_qos_entity(struct rq *rq,
 						   struct qos_rq *qos_rq)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 	return NULL;
 }
+#endif
 
 static struct task_struct *
 pick_next_task_qos(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 {
 	struct task_struct *p = NULL;
+	MEDEBUG("zz %s %d %s\n", __func__, __LINE__, current->comm);
 	return p;
 }
 
 static void put_prev_task_qos(struct rq *rq, struct task_struct *p)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
 /* Assumes rq->lock is held */
 static void rq_online_qos(struct rq *rq)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
+	cpupri_set(&rq->rd->cpupri, rq->cpu, rq->qos.highest_prio.curr);
 }
 
 static void rq_offline_qos(struct rq *rq)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
+	cpupri_set(&rq->rd->cpupri, rq->cpu, CPUPRI_INVALID);
 }
 
 static void task_woken_qos(struct rq *rq, struct task_struct *p)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
 static void switched_from_qos(struct rq *rq, struct task_struct *p)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
 static void set_curr_task_qos(struct rq *rq)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
 static void task_tick_qos(struct rq *rq, struct task_struct *p, int queued)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
 static unsigned int get_rr_interval_qos(struct rq *rq, struct task_struct *task)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 	return 0;
 }
 
@@ -141,25 +214,32 @@ static unsigned int get_rr_interval_qos(struct rq *rq, struct task_struct *task)
 static void
 prio_changed_qos(struct rq *rq, struct task_struct *p, int oldprio)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
 
 static void switched_to_qos(struct rq *rq, struct task_struct *p)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 
 }
 
 static void update_curr_qos(struct rq *rq)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 
 }
 
+#ifdef CONFIG_SCHED_SLI
 static void update_nr_uninterruptible_qos(struct task_struct *p, long inc)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 }
+#endif
 
 static int
 select_task_rq_qos(struct task_struct *p, int cpu, int sd_flag, int flags)
 {
+	MEDEBUG("zz %s %d \n", __func__, __LINE__);
 	return cpu;
 }
 
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 3d1792ad4564..5c3a022ad862 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -2458,7 +2458,8 @@ static void update_nr_uninterruptible_rt(struct task_struct *p, long inc)
 #endif
 
 const struct sched_class rt_sched_class = {
-	.next			= &fair_sched_class,
+	//.next			= &fair_sched_class,
+	.next			= &qos_sched_class,
 	.enqueue_task		= enqueue_task_rt,
 	.dequeue_task		= dequeue_task_rt,
 	.yield_task		= yield_task_rt,
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0b8cd607091d..e106b5d9d4f2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -164,6 +164,10 @@ extern long calc_load_fold_active(struct rq *this_rq, long adjust);
  */
 #define RUNTIME_INF		((u64)~0ULL)
 
+static inline int qos_policy(int policy)
+{
+	return policy == SCHED_QOS;
+}
 static inline int idle_policy(int policy)
 {
 	return policy == SCHED_IDLE;
@@ -185,9 +189,13 @@ static inline int dl_policy(int policy)
 static inline bool valid_policy(int policy)
 {
 	return idle_policy(policy) || fair_policy(policy) ||
-		rt_policy(policy) || dl_policy(policy);
+		rt_policy(policy) || dl_policy(policy) || qos_policy(policy);
 }
 
+static inline int task_has_qos_policy(struct task_struct *p)
+{
+	return qos_policy(p->policy);
+}
 static inline int task_has_rt_policy(struct task_struct *p)
 {
 	return rt_policy(p->policy);
@@ -233,6 +241,11 @@ dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
 	       dl_time_before(a->deadline, b->deadline);
 }
 
+struct qos_prio_array {
+	DECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* include 1 bit for delimiter */
+	struct list_head queue[MAX_RT_PRIO];
+};
+
 /*
  * This is the priority-queue data structure of the RT scheduling class:
  */
@@ -336,6 +349,7 @@ extern bool dl_cpu_busy(unsigned int cpu);
 
 struct cfs_rq;
 struct rt_rq;
+struct qos_rq;
 
 extern struct list_head task_groups;
 
@@ -399,6 +413,7 @@ struct task_group {
 #ifdef CONFIG_RT_GROUP_SCHED
 	struct sched_rt_entity	**rt_se;
 	struct rt_rq		**rt_rq;
+	struct qos_rq		**qos_rq;
 
 	struct rt_bandwidth	rt_bandwidth;
 #endif
@@ -676,6 +691,37 @@ struct rt_rq {
 	unsigned long nr_uninterruptible;
 };
 
+/* Real-Time classes' related field in a runqueue: */
+struct qos_rq {
+	struct qos_prio_array	active;
+	unsigned int		qos_nr_running;
+	unsigned int		rr_nr_running;
+	struct {
+		int		curr; /* highest queued qos task prio */
+		int		next; /* next highest */
+	} highest_prio;
+
+	unsigned long		qos_nr_migratory;
+	unsigned long		qos_nr_total;
+	int			overloaded;
+	struct plist_head	pushable_tasks;
+
+	int			qos_queued;
+
+	int			qos_throttled;
+	u64			qos_time;
+	u64			qos_runtime;
+	/* Nests inside the rq lock: */
+	raw_spinlock_t		qos_runtime_lock;
+
+	unsigned long		qos_nr_boosted;
+
+	struct rq		*rq;
+	struct task_group	*tg;
+
+	unsigned long nr_uninterruptible;
+};
+
 static inline bool rt_rq_is_runnable(struct rt_rq *rt_rq)
 {
 	return rt_rq->rt_queued && rt_rq->rt_nr_running;
@@ -864,6 +910,7 @@ struct rq {
 
 	struct cfs_rq		cfs;
 	struct rt_rq		rt;
+	struct rt_rq		qos;
 	struct dl_rq		dl;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -1786,6 +1833,7 @@ extern const struct sched_class dl_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
+extern const struct sched_class qos_sched_class;
 
 
 #ifdef CONFIG_SMP
-- 
2.25.1

